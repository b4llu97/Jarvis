# üöÄ Schnellstart: LLM-Upgrade

## In 3 Schritten zu besserer KI

### 1Ô∏è‚É£ Konfiguration erstellen

```bash
cd /home/ubuntu/Jarvis

# Kopiere die Beispiel-Konfiguration
cp .env.example config/.env

# Bearbeite die Datei (optional - funktioniert auch ohne √Ñnderungen)
nano config/.env
```

**Wichtig:** Der `OPENAI_API_KEY` ist bereits vom System konfiguriert! Du musst nichts √§ndern.

### 2Ô∏è‚É£ Services starten

```bash
# Stoppe alte Services
docker-compose down

# Baue neue Services
docker-compose build llm_gateway orchestrator

# Starte alles
docker-compose up -d

# Warte ~30 Sekunden, dann pr√ºfe Status
docker-compose ps
```

### 3Ô∏è‚É£ Testen

```bash
# Health Check
curl http://localhost:8007/health

# Test-Anfrage
curl -X POST http://localhost:8003/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query":"Erkl√§re mir in einem Satz, was du jetzt besser kannst als vorher."}'
```

## ‚úÖ Fertig!

Dein JARVIS nutzt jetzt **gpt-4.1-mini** als prim√§res Modell mit automatischem Fallback zu lokalem Ollama.

### Was ist neu?

- üß† **Deutlich intelligentere Antworten**
- üí¨ **Nat√ºrlichere Konversation**
- üîÑ **Automatisches Fallback** bei API-Problemen
- ‚öôÔ∏è **Einfacher Provider-Wechsel** per Umgebungsvariable

### Verf√ºgbare Modelle

In `config/.env` kannst du w√§hlen:

```bash
# Schnell & g√ºnstig (Standard)
PRIMARY_LLM_MODEL=gpt-4.1-mini

# Sehr schnell & sehr g√ºnstig
PRIMARY_LLM_MODEL=gpt-4.1-nano

# Google Gemini
PRIMARY_LLM_MODEL=gemini-2.5-flash

# Nur lokales Ollama (keine API)
PRIMARY_LLM_PROVIDER=ollama
PRIMARY_LLM_MODEL=llama3.1
```

Nach √Ñnderungen:
```bash
docker-compose restart llm_gateway orchestrator
```

## üéØ N√§chste Schritte

Siehe `LLM_UPGRADE_README.md` f√ºr:
- Detaillierte Konfiguration
- Troubleshooting
- Kosten-√úbersicht
- Erweiterte Features

**Viel Spa√ü mit deinem upgegradeten JARVIS!** üéâ
